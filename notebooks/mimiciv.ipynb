{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "sys.path.append('../')\n",
    "from src.plots import add_panel_text\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.constants import *\n",
    "from pydts.examples_utils.plots import plot_example_pred_output\n",
    "from pydts.examples_utils.plots import add_panel_text\n",
    "from pydts.fitters import TwoStagesFitter, DataExpansionFitter\n",
    "from pydts.examples_utils.plots import plot_events_occurrence\n",
    "from pydts.cross_validation import TwoStagesCV\n",
    "import pickle\n",
    "from tableone import TableOne\n",
    "from time import time\n",
    "\n",
    "slicer = pd.IndexSlice\n",
    "\n",
    "\n",
    "OUTPUT_DIR = '/app/output'\n",
    "DATA_DIR = '/app/data/mimic-iv-2.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "patients_file = os.path.join(DATA_DIR, 'hosp', 'patients.csv.gz')\n",
    "admissions_file = os.path.join(DATA_DIR, 'hosp', 'admissions.csv.gz')\n",
    "lab_file = os.path.join(DATA_DIR, 'hosp', 'labevents.csv.gz')\n",
    "lab_meta_file = os.path.join(DATA_DIR, 'hosp', 'd_labitems.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "patients_df = pd.read_csv(patients_file, compression='gzip')\n",
    "patients_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = ['dod']\n",
    "patients_df.drop(COLUMNS_TO_DROP, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(patients_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "patients_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "admissions_df = pd.read_csv(admissions_file, compression='gzip', parse_dates=[ADMISSION_TIME_COL,\n",
    "                            DISCHARGE_TIME_COL, DEATH_TIME_COL, ED_REG_TIME, ED_OUT_TIME])\n",
    "admissions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = ['hospital_expire_flag', 'edouttime', 'edregtime', 'deathtime', 'language']\n",
    "admissions_df.drop(COLUMNS_TO_DROP, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "admissions_df = admissions_df.merge(patients_df, on=[SUBJECT_ID_COL])\n",
    "admissions_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Calculate Age at Admission and Group of Admission Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on mimic IV example https://mimic.mit.edu/docs/iv/modules/hosp/patients/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Diff column first\n",
    "admissions_df[ADMISSION_YEAR_COL] = (admissions_df[ADMISSION_TIME_COL].dt.year - admissions_df['anchor_year'])\n",
    "\n",
    "# Age at admission calculation\n",
    "admissions_df[ADMISSION_AGE_COL] = (admissions_df[AGE_COL] + admissions_df[ADMISSION_YEAR_COL])\n",
    "\n",
    "# Admission year group lower bound calculation\n",
    "admissions_df[ADMISSION_YEAR_COL] = admissions_df[ADMISSION_YEAR_COL] + admissions_df[YEAR_GROUP_COL].apply(lambda x: int(x.split(' ')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "admissions_df[ADMISSION_YEAR_COL].value_counts().sort_index().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Admission Year (lower bound)', fontsize=font_sz)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,4))\n",
    "tmp = admissions_df[[ADMISSION_AGE_COL, GENDER_COL]]\n",
    "tmp.groupby([ADMISSION_AGE_COL, GENDER_COL]).size().unstack().plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Age at Admission [years]', fontsize=font_sz)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_title(f'Total Population, N={len(tmp)}', fontsize=font_sz)\n",
    "ax.legend(labels=['Female', 'Male'], title=\"Sex\")\n",
    "plt.setp(ax.get_xticklabels()[1::2], visible=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Calculating LOS (exact, days resolution) and night admission indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "admissions_df[LOS_EXACT_COL] = (admissions_df[DISCHARGE_TIME_COL] - admissions_df[ADMISSION_TIME_COL])\n",
    "admissions_df[NIGHT_ADMISSION_FLAG] = ((admissions_df[ADMISSION_TIME_COL].dt.hour >= 20) | \\\n",
    "                                       (admissions_df[ADMISSION_TIME_COL].dt.hour < 8) ).values\n",
    "admissions_df[LOS_DAYS_COL] = admissions_df[LOS_EXACT_COL].dt.ceil('1d')\n",
    "print(f\"Mean night admissions flag: {admissions_df[NIGHT_ADMISSION_FLAG].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "admissions_df[ADMISSION_TYPE_COL].value_counts().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Admission Type', fontsize=font_sz)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_clip_days = 28\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "tmp = admissions_df[admissions_df[ADMISSION_TYPE_COL] == 'URGENT']\n",
    "los_bar = tmp[LOS_DAYS_COL].clip(pd.to_timedelta('1d'), pd.to_timedelta(f'{max_clip_days}d')).value_counts().sort_index()\n",
    "los_bar.index = np.arange(1, max_clip_days+1)\n",
    "los_bar.plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('LOS (Days)', fontsize=font_sz)\n",
    "ax.grid(axis='y')\n",
    "ax.set_title('URGENT', fontsize=font_sz)\n",
    "\n",
    "ax = axes[1]\n",
    "tmp = admissions_df[admissions_df[ADMISSION_TYPE_COL] == 'EW EMER.']\n",
    "los_bar = tmp[LOS_DAYS_COL].clip(pd.to_timedelta('1d'), pd.to_timedelta(f'{max_clip_days}d')).value_counts().sort_index()\n",
    "los_bar.index = np.arange(1, max_clip_days+1)\n",
    "los_bar.plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('LOS (Days)', fontsize=font_sz)\n",
    "ax.grid(axis='y')\n",
    "ax.set_title('EW EMER.', fontsize=font_sz)\n",
    "\n",
    "ax = axes[2]\n",
    "tmp = admissions_df[admissions_df[ADMISSION_TYPE_COL] == 'DIRECT EMER.']\n",
    "los_bar = tmp[LOS_DAYS_COL].clip(pd.to_timedelta('1d'), pd.to_timedelta(f'{max_clip_days}d')).value_counts().sort_index()\n",
    "los_bar.index = np.arange(1, max_clip_days+1)\n",
    "los_bar.plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('LOS (Days)', fontsize=font_sz)\n",
    "ax.grid(axis='y')\n",
    "ax.set_title('DIRECT EMER.', fontsize=font_sz)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Taking only SPECIFIC_ADMISSION_TYPE admissions from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SPECIFIC_ADMISSION_TYPE = ['DIRECT EMER.', 'EW EMER.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(admissions_df))\n",
    "admissions_df = admissions_df[admissions_df[ADMISSION_TYPE_COL].isin(SPECIFIC_ADMISSION_TYPE)]\n",
    "print(len(admissions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add direct emergency if needed\n",
    "\n",
    "if 'DIRECT EMER.' in SPECIFIC_ADMISSION_TYPE:\n",
    "    admissions_df[DIRECT_IND_COL] = (admissions_df[ADMISSION_TYPE_COL] == 'DIRECT EMER.').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Counting SPECIFIC_ADMISSION_TYPE admissions to each patient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "number_of_admissions = admissions_df.groupby(SUBJECT_ID_COL)[ADMISSION_ID_COL].nunique()\n",
    "number_of_admissions.name = ADMISSION_COUNT_COL\n",
    "number_of_admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "number_of_admissions.value_counts().sort_index().plot.bar(ax=ax, logy=True)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Number of Admissions', fontsize=font_sz)\n",
    "ax.grid('y', which='minor', alpha=0.4)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "admissions_df = admissions_df.merge(number_of_admissions, on=SUBJECT_ID_COL)\n",
    "admissions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Add recurrent admissions group per patient according to last admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ADMISSION_COUNT_BINS = [1, 1.5, 2.5, 5000]\n",
    "ADMISSION_COUNT_LABELS = ['1', '2', '3up']\n",
    "\n",
    "admissions_df[ADMISSION_COUNT_GROUP_COL] = pd.cut(admissions_df[ADMISSION_COUNT_COL], \n",
    "                                                  bins=ADMISSION_COUNT_BINS, \n",
    "                                                  labels=ADMISSION_COUNT_LABELS, \n",
    "                                                  include_lowest=True)\n",
    "admissions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Adds last admission with previous admission in past month indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indicator_diff = pd.to_timedelta('30d')\n",
    "\n",
    "tmp_admissions = admissions_df[admissions_df[ADMISSION_COUNT_COL] > 1]\n",
    "print(tmp_admissions.shape)\n",
    "ind_ser = tmp_admissions.sort_values(by=[SUBJECT_ID_COL, ADMISSION_TIME_COL]).groupby(\n",
    "    SUBJECT_ID_COL).apply(\n",
    "    lambda tmp_df: (tmp_df[ADMISSION_TIME_COL] - tmp_df[DISCHARGE_TIME_COL].shift(1)) <= indicator_diff)\n",
    "\n",
    "ind_ser.index = ind_ser.index.droplevel(1)\n",
    "ind_ser.name = PREV_ADMISSION_IND_COL\n",
    "ind_ser = ind_ser.iloc[ind_ser.reset_index().drop_duplicates(subset=[SUBJECT_ID_COL], keep='last').index]\n",
    "ind_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "admissions_df = admissions_df.merge(ind_ser.astype(int), left_on=SUBJECT_ID_COL, right_index=True, how='outer')\n",
    "admissions_df[PREV_ADMISSION_IND_COL].fillna(0, inplace=True)\n",
    "admissions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "admissions_df[admissions_df[PREV_ADMISSION_IND_COL] == 1].sort_values(by=[SUBJECT_ID_COL, ADMISSION_TIME_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Keep only last admission per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "only_last_admission = admissions_df.sort_values(by=[ADMISSION_TIME_COL]).drop_duplicates(subset=[SUBJECT_ID_COL], keep='last')\n",
    "len(only_last_admission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Take only patients with last admission after MINIMUM YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MINIMUM_YEAR = 2017\n",
    "MINIMUM_YEAR = 2014\n",
    "print(len(only_last_admission))\n",
    "only_last_admission = only_last_admission[only_last_admission[ADMISSION_YEAR_COL] >= MINIMUM_YEAR]\n",
    "print(len(only_last_admission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "only_last_admission[PREV_ADMISSION_IND_COL].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pids = only_last_admission[SUBJECT_ID_COL].drop_duplicates()\n",
    "adm_ids = only_last_admission[ADMISSION_ID_COL].drop_duplicates()\n",
    "print(len(pids))\n",
    "print(len(adm_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load relevant lab tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOAD_SPECIFIC_COLUMNS = [SUBJECT_ID_COL, ADMISSION_ID_COL, ITEM_ID_COL, 'storetime', 'flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chunksize = 10 ** 6\n",
    "full_df = pd.DataFrame()\n",
    "with pd.read_csv(lab_file, chunksize=chunksize, compression='gzip', parse_dates=[STORE_TIME_COL], usecols=LOAD_SPECIFIC_COLUMNS) as reader:\n",
    "    for chunk in reader:\n",
    "        tmp_chunk = chunk[chunk[SUBJECT_ID_COL].isin(pids) & chunk[ADMISSION_ID_COL].isin(adm_ids)]\n",
    "        tmp_adms = only_last_admission[only_last_admission[SUBJECT_ID_COL].isin(pids) & only_last_admission[ADMISSION_ID_COL].isin(adm_ids)]\n",
    "        #tmp_patinets = patients_df[patients_df[SUBJECT_ID_COL].isin(pids)]\n",
    "        tmp_chunk = tmp_chunk.merge(tmp_adms, on=[SUBJECT_ID_COL, ADMISSION_ID_COL])\n",
    "        #tmp = tmp_chunk.merge(tmp_patinets, on=[SUBJECT_ID_COL])\n",
    "        full_df = pd.concat([full_df, tmp_chunk])\n",
    "        print(len(full_df))\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Continue only with included patients_df and admissions_df and full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pids = full_df[SUBJECT_ID_COL].drop_duplicates().values\n",
    "adms_ids = full_df[ADMISSION_ID_COL].drop_duplicates().values\n",
    "print(len(patients_df))\n",
    "patients_df = patients_df[patients_df[SUBJECT_ID_COL].isin(pids)]\n",
    "print(len(patients_df))\n",
    "print(len(admissions_df))\n",
    "admissions_df = admissions_df[admissions_df[ADMISSION_ID_COL].isin(adms_ids)]\n",
    "print(len(admissions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "admissions_df[ADMISSION_LOCATION_COL].value_counts().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Admission Location', fontsize=font_sz)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "admissions_df[DISCHARGE_LOCATION_COL].value_counts().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Discharge Location', fontsize=font_sz)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regrouping discharge location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "discharge_regrouping_df = pd.Series(DISCHARGE_REGROUPING_DICT).to_frame()\n",
    "discharge_regrouping_df.index.name = 'Original Group'\n",
    "discharge_regrouping_df.columns = ['Regrouped']\n",
    "discharge_regrouping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "admissions_df[DISCHARGE_LOCATION_COL].replace(DISCHARGE_REGROUPING_DICT, inplace=True)\n",
    "full_df[DISCHARGE_LOCATION_COL].replace(DISCHARGE_REGROUPING_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "admissions_df[DISCHARGE_LOCATION_COL].value_counts().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Discharge Location', fontsize=font_sz)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "tmp = admissions_df[[ADMISSION_AGE_COL, GENDER_COL]]\n",
    "tmp.groupby([ADMISSION_AGE_COL, GENDER_COL]).size().unstack().plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Age at Admission [years]', fontsize=font_sz)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_title(f'Total Population, N={len(tmp)}', fontsize=font_sz)\n",
    "ax.legend(labels=['Female', 'Male'], title=\"Sex\")\n",
    "plt.setp(ax.get_xticklabels()[1::2], visible=False)\n",
    "fig.savefig(os.path.join(OUTPUT_DIR, 'age_gender_admissions_subset.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regroup Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "race_regrouping_df = pd.Series(RACE_REGROUPING_DICT).to_frame()\n",
    "race_regrouping_df.index.name = 'Original Group'\n",
    "race_regrouping_df.columns = ['Regrouped']\n",
    "race_regrouping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "admissions_df[RACE_COL].replace(RACE_REGROUPING_DICT, inplace=True)\n",
    "full_df[RACE_COL].replace(RACE_REGROUPING_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "admissions_df[RACE_COL].value_counts().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Race', fontsize=font_sz)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,dpi=100)\n",
    "admissions_df[INSURANCE_COL].value_counts().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Number of Patients', fontsize=font_sz)\n",
    "ax.set_xlabel('Insurance', fontsize=font_sz)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 1.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Taking only results 24 hours from admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_df[ADMISSION_TO_RESULT_COL] = (full_df[STORE_TIME_COL] - full_df[ADMISSION_TIME_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_df = full_df[full_df[ADMISSION_TO_RESULT_COL] <= pd.to_timedelta('1d')]\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(full_df))\n",
    "full_df.sort_values(by=[ADMISSION_TIME_COL, STORE_TIME_COL]).drop_duplicates(subset=[SUBJECT_ID_COL, ADMISSION_ID_COL, ITEM_ID_COL], \n",
    "    inplace=True, keep='last')\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Most common lab tests upon arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lab_meta_df = pd.read_csv(lab_meta_file, compression='gzip')\n",
    "lab_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "common_tests = full_df.groupby(ITEM_ID_COL)[ADMISSION_ID_COL].nunique().sort_values(ascending=False)\n",
    "included_in_threshold = common_tests[common_tests > threshold].to_frame().merge(lab_meta_df, on=ITEM_ID_COL)\n",
    "included_in_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(full_df))\n",
    "full_df = full_df[full_df[ITEM_ID_COL].isin(included_in_threshold[ITEM_ID_COL].values)]\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minimal_item_id = included_in_threshold.iloc[-1][ITEM_ID_COL]\n",
    "minimal_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pids = full_df[full_df[ITEM_ID_COL] == minimal_item_id][SUBJECT_ID_COL].drop_duplicates().values\n",
    "adms_ids = full_df[full_df[ITEM_ID_COL] == minimal_item_id][ADMISSION_ID_COL].drop_duplicates().values\n",
    "print(len(patients_df))\n",
    "patients_df = patients_df[patients_df[SUBJECT_ID_COL].isin(pids)]\n",
    "print(len(patients_df))\n",
    "print(len(admissions_df))\n",
    "admissions_df = admissions_df[admissions_df[ADMISSION_ID_COL].isin(adms_ids)]\n",
    "print(len(admissions_df))\n",
    "print(len(admissions_df))\n",
    "full_df = full_df[full_df[SUBJECT_ID_COL].isin(pids)]\n",
    "full_df = full_df[full_df[ADMISSION_ID_COL].isin(adms_ids)]\n",
    "print(len(admissions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_df['flag'].fillna('normal', inplace=True)\n",
    "full_df['flag'].replace({'normal': 0, 'abnormal':1}, inplace=True)\n",
    "full_df['flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_df = full_df.sort_values(by=[ADMISSION_TIME_COL, STORE_TIME_COL]).drop_duplicates(\n",
    "    subset=[SUBJECT_ID_COL, ADMISSION_ID_COL, ITEM_ID_COL], \n",
    "    keep='last')\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = full_df[[SUBJECT_ID_COL, ADMISSION_ID_COL, ITEM_ID_COL, 'flag']]\n",
    "fitters_table = pd.pivot_table(tmp, values=['flag'], index=[SUBJECT_ID_COL, ADMISSION_ID_COL], \n",
    "                               columns=[ITEM_ID_COL], aggfunc=np.sum)\n",
    "fitters_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitters_table = fitters_table.droplevel(1, axis=0).droplevel(0, axis=1)\n",
    "fitters_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dummies_df = full_df.drop_duplicates(subset=[SUBJECT_ID_COL]).set_index(SUBJECT_ID_COL)\n",
    "dummies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del full_df\n",
    "del admissions_df\n",
    "del patients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Standardize age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "dummies_df[STANDARDIZED_AGE_COL] = scaler.fit_transform(dummies_df[[AGE_COL]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "J_DICT = {'HOME': 1, 'FURTHER TREATMENT': 2, 'DIED': 3, 'CENSORED': 0} \n",
    "GENDER_DICT = {'F': 1, 'M': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dummies_df[GENDER_COL] = dummies_df[GENDER_COL].replace(GENDER_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "included_in_threshold['label'] = included_in_threshold['label'].apply(lambda x: x.replace(' ', '')).apply(lambda x: x.replace(',', ''))\n",
    "RENAME_ITEMS_DICT = included_in_threshold[[ITEM_ID_COL, 'label']].set_index(ITEM_ID_COL).to_dict()['label']\n",
    "RENAME_ITEMS_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "table1 = pd.concat([\n",
    "    fitters_table.copy(),\n",
    "    dummies_df[[NIGHT_ADMISSION_FLAG,\n",
    "                GENDER_COL, \n",
    "                DIRECT_IND_COL,\n",
    "                PREV_ADMISSION_IND_COL,\n",
    "                ADMISSION_AGE_COL]].astype(int),\n",
    "    dummies_df[[INSURANCE_COL,\n",
    "                MARITAL_STATUS_COL,\n",
    "                RACE_COL,\n",
    "                ADMISSION_COUNT_GROUP_COL]],\n",
    "    dummies_df[LOS_DAYS_COL].dt.days,\n",
    "    dummies_df[DISCHARGE_LOCATION_COL].dropna().replace(J_DICT).astype(int)\n",
    "], axis=1)\n",
    "    \n",
    "table1.rename(RENAME_ITEMS_DICT, inplace=True, axis=1)  \n",
    "table1.dropna(inplace=True)\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADMINISTRATIVE_CENSORING = 28\n",
    "censoring_index = table1[table1[LOS_DAYS_COL] > ADMINISTRATIVE_CENSORING].index\n",
    "table1.loc[censoring_index, DISCHARGE_LOCATION_COL] = 0\n",
    "table1.loc[censoring_index, LOS_DAYS_COL] = ADMINISTRATIVE_CENSORING + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1[GENDER_COL].replace(table1_rename_sex, inplace=True)\n",
    "table1[RACE_COL].replace(table1_rename_race, inplace=True)\n",
    "table1[MARITAL_STATUS_COL].replace(table1_rename_marital, inplace=True)\n",
    "table1[DIRECT_IND_COL].replace(table1_rename_yes_no, inplace=True)\n",
    "table1[NIGHT_ADMISSION_FLAG].replace(table1_rename_yes_no, inplace=True)\n",
    "table1[PREV_ADMISSION_IND_COL].replace(table1_rename_yes_no, inplace=True)\n",
    "table1[DISCHARGE_LOCATION_COL].replace(table1_rename_discharge, inplace=True)\n",
    "table1[ADMISSION_COUNT_GROUP_COL].replace({'3up': '3+'}, inplace=True)\n",
    "table1.rename(table1_rename_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['gender', 'admission_age', 'race', 'insurance', 'marital_status',\n",
    "           'direct_emrgency_flag', 'night_admission', 'last_less_than_diff', \n",
    "           'admissions_count_group', 'LOS days', 'discharge_location']\n",
    "columns = [table1_rename_columns[c] for c in columns]\n",
    "categorical = ['gender', 'race', 'insurance', 'marital_status',\n",
    "           'direct_emrgency_flag', 'night_admission', 'last_less_than_diff', \n",
    "           'admissions_count_group', 'discharge_location']\n",
    "categorical = [table1_rename_columns[c] for c in categorical]\n",
    "table1.dropna(inplace=True)\n",
    "groupby = [table1_rename_columns[DISCHARGE_LOCATION_COL]]\n",
    "mytable = TableOne(table1, columns, categorical, groupby, missing=False)\n",
    "mytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(mytable.tableone.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = [DISCHARGE_LOCATION_COL, 'AnionGap', 'Bicarbonate', 'CalciumTotal', 'Chloride', 'Creatinine',\n",
    "           'Glucose', 'Magnesium', 'Phosphate', 'Potassium', 'Sodium',\n",
    "           'UreaNitrogen', 'Hematocrit', 'Hemoglobin', 'MCH', 'MCHC', 'MCV',\n",
    "           'PlateletCount', 'RDW', 'RedBloodCells', 'WhiteBloodCells']\n",
    "categorical = [DISCHARGE_LOCATION_COL, 'AnionGap', 'Bicarbonate', 'CalciumTotal', 'Chloride', 'Creatinine',\n",
    "           'Glucose', 'Magnesium', 'Phosphate', 'Potassium', 'Sodium',\n",
    "           'UreaNitrogen', 'Hematocrit', 'Hemoglobin', 'MCH', 'MCHC', 'MCV',\n",
    "           'PlateletCount', 'RDW', 'RedBloodCells', 'WhiteBloodCells']\n",
    "columns = [table1_rename_columns[c] for c in columns]\n",
    "categorical = [table1_rename_columns[c] for c in categorical]\n",
    "groupby = [table1_rename_columns[DISCHARGE_LOCATION_COL]]\n",
    "mytable = TableOne(table1.dropna().replace(table1_rename_normal_abnormal), columns, categorical, groupby, missing=False)\n",
    "mytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(mytable.tableone.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitters_table = pd.concat([\n",
    "    fitters_table.copy(),\n",
    "    pd.get_dummies(dummies_df[INSURANCE_COL], prefix='Insurance', drop_first=True),\n",
    "    pd.get_dummies(dummies_df[MARITAL_STATUS_COL], prefix='Marital', drop_first=True),\n",
    "    pd.get_dummies(dummies_df[RACE_COL], prefix='Ethnicity', drop_first=True),\n",
    "    pd.get_dummies(dummies_df[ADMISSION_COUNT_GROUP_COL], prefix='AdmsCount', drop_first=True),\n",
    "    dummies_df[[NIGHT_ADMISSION_FLAG, \n",
    "                GENDER_COL, \n",
    "                DIRECT_IND_COL,\n",
    "                PREV_ADMISSION_IND_COL]].astype(int),\n",
    "    dummies_df[STANDARDIZED_AGE_COL],\n",
    "    dummies_df[LOS_DAYS_COL].dt.days,\n",
    "    dummies_df[DISCHARGE_LOCATION_COL].dropna().replace(J_DICT).astype(int)\n",
    "], axis=1)\n",
    "    \n",
    "fitters_table   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitters_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(fitters_table))\n",
    "fitters_table.dropna(inplace=True)\n",
    "fitters_table = fitters_table[fitters_table.index.isin(table1.index)]\n",
    "print(len(fitters_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitters_table.reset_index(inplace=True)\n",
    "fitters_table.rename({DISCHARGE_LOCATION_COL: 'J', LOS_DAYS_COL: 'X', SUBJECT_ID_COL: 'pid'}, inplace=True, axis=1)\n",
    "fitters_table.rename(RENAME_ITEMS_DICT, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitters_table = fitters_table[fitters_table['X'] > 0]\n",
    "fitters_table.loc[fitters_table.X > ADMINISTRATIVE_CENSORING, 'J'] = 0\n",
    "fitters_table.loc[fitters_table.X > ADMINISTRATIVE_CENSORING, 'X'] = ADMINISTRATIVE_CENSORING + 1\n",
    "fitters_table['J'] = fitters_table['J'].astype(int)\n",
    "\n",
    "plot_events_occurrence(fitters_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "case = f'mimic_final_'\n",
    "two_step_timing = []\n",
    "lee_timing = []\n",
    "\n",
    "# Two step fitter\n",
    "new_fitter = TwoStagesFitter()\n",
    "print(f'Starting two-step')\n",
    "two_step_start = time()\n",
    "new_fitter.fit(df=fitters_table, nb_workers=1)\n",
    "two_step_end = time()\n",
    "print(f'Finished two-step: {two_step_end-two_step_start}sec')\n",
    "\n",
    "two_step_timing.append(two_step_end-two_step_start)\n",
    "\n",
    "# Lee et al fitter\n",
    "print(f'Starting Lee et al.')\n",
    "lee_fitter = DataExpansionFitter()\n",
    "lee_start = time()\n",
    "lee_fitter.fit(df=fitters_table)\n",
    "lee_end = time()\n",
    "print(f'Finished lee: {lee_end-lee_start}sec')\n",
    "\n",
    "lee_timing.append(lee_end-lee_start) \n",
    "\n",
    "# Regularized Two step fitter\n",
    "reg_fitter = TwoStagesFitter()\n",
    "print(f'Starting regularized two-step')\n",
    "fit_beta_kwargs = {\n",
    "        'model_kwargs': {\n",
    "        'penalizer': np.exp(-7),\n",
    "        'l1_ratio': 1\n",
    "    }\n",
    "}\n",
    "reg_two_step_start = time()\n",
    "reg_fitter.fit(df=fitters_table, nb_workers=1, fit_beta_kwargs=fit_beta_kwargs)\n",
    "reg_two_step_end = time()\n",
    "print(f'Finished two-step: {reg_two_step_end-reg_two_step_start}sec')\n",
    "\n",
    "lee_alpha_ser = lee_fitter.get_alpha_df().loc[:, slicer[:, [COEF_COL, STDERR_COL] ]].unstack().sort_index()\n",
    "lee_beta_ser = lee_fitter.get_beta_SE().loc[:, slicer[:, [COEF_COL, STDERR_COL] ]].unstack().sort_index()\n",
    "\n",
    "two_step_alpha_k_results = new_fitter.alpha_df[['J', 'X', 'alpha_jt']]\n",
    "two_step_beta_k_results = new_fitter.get_beta_SE().unstack().to_frame()\n",
    "\n",
    "reg_two_step_alpha_k_results = reg_fitter.alpha_df[['J', 'X', 'alpha_jt']]\n",
    "reg_two_step_beta_k_results = reg_fitter.get_beta_SE().unstack().to_frame()\n",
    "\n",
    "lee_alpha_k_results = lee_alpha_ser.to_frame()\n",
    "lee_beta_k_results = lee_beta_ser.to_frame()\n",
    "\n",
    "# Cache results\n",
    "two_step_alpha_k_results.to_csv(os.path.join(OUTPUT_DIR, f'{case}_two_step_alpha.csv'))\n",
    "two_step_beta_k_results.to_csv(os.path.join(OUTPUT_DIR, f'{case}_two_step_beta.csv'))\n",
    "reg_two_step_alpha_k_results.to_csv(os.path.join(OUTPUT_DIR, f'{case}_reg_two_step_alpha.csv'))\n",
    "reg_two_step_beta_k_results.to_csv(os.path.join(OUTPUT_DIR, f'{case}_reg_two_step_beta.csv'))\n",
    "lee_alpha_k_results.to_csv(os.path.join(OUTPUT_DIR, f'{case}_lee_alpha.csv'))\n",
    "lee_beta_k_results.to_csv(os.path.join(OUTPUT_DIR, f'{case}_lee_beta.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "covariates = [c for c in fitters_table.columns if c not in ['pid', 'J', 'X']]\n",
    "covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "two_step_alpha_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_two_step_alpha.csv'), \n",
    "                                       index_col=['J', 'X'])\n",
    "two_step_beta_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_two_step_beta.csv'),\n",
    "                                      index_col=[0, 1])\n",
    "reg_two_step_alpha_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_reg_two_step_alpha.csv'), \n",
    "                                       index_col=['J', 'X'])\n",
    "reg_two_step_beta_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_reg_two_step_beta.csv'),\n",
    "                                      index_col=[0, 1])\n",
    "lee_alpha_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_lee_alpha.csv'),\n",
    "                                  index_col=[0,1,2])\n",
    "lee_beta_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_lee_beta.csv'),\n",
    "                                 index_col=[0, 1, 2])\n",
    "\n",
    "\n",
    "twostep_beta1_summary = two_step_beta_k_results.mean(axis=1).unstack([0]).round(3).iloc[:, [1,0]]\n",
    "twostep_beta1_summary.index = [f'{iii.replace(\" \", \"\")}_1' for iii in twostep_beta1_summary.index]\n",
    "twostep_beta2_summary = two_step_beta_k_results.mean(axis=1).unstack([0]).round(3).iloc[:, [3,2]]\n",
    "twostep_beta2_summary.index = [f'{iii.replace(\" \", \"\")}_2' for iii in twostep_beta2_summary.index]\n",
    "twostep_beta3_summary = two_step_beta_k_results.mean(axis=1).unstack([0]).round(3).iloc[:, [5,4]]\n",
    "twostep_beta3_summary.index = [f'{iii.replace(\" \", \"\")}_3' for iii in twostep_beta3_summary.index]\n",
    "\n",
    "reg_twostep_beta1_summary = reg_two_step_beta_k_results.mean(axis=1).unstack([0]).round(3).iloc[:, [1,0]]\n",
    "reg_twostep_beta1_summary.index = [f'{iii.replace(\" \", \"\")}_1' for iii in reg_twostep_beta1_summary.index]\n",
    "reg_twostep_beta2_summary = reg_two_step_beta_k_results.mean(axis=1).unstack([0]).round(3).iloc[:, [3,2]]\n",
    "reg_twostep_beta2_summary.index = [f'{iii.replace(\" \", \"\")}_2' for iii in reg_twostep_beta2_summary.index]\n",
    "reg_twostep_beta3_summary = reg_two_step_beta_k_results.mean(axis=1).unstack([0]).round(3).iloc[:, [5,4]]\n",
    "reg_twostep_beta3_summary.index = [f'{iii.replace(\" \", \"\")}_3' for iii in reg_twostep_beta3_summary.index]\n",
    "\n",
    "lee_beta1_summary = lee_beta_k_results.mean(axis=1).loc[slicer[1,:,:]].unstack([0]).round(3)\n",
    "lee_beta1_summary.index = [f'{iii.replace(\" \", \"\")}_1' for iii in lee_beta1_summary.index]\n",
    "lee_beta2_summary = lee_beta_k_results.mean(axis=1).loc[slicer[2,:,:]].unstack([0]).round(3)\n",
    "lee_beta2_summary.index = [f'{iii.replace(\" \", \"\")}_2' for iii in lee_beta2_summary.index]\n",
    "lee_beta3_summary = lee_beta_k_results.mean(axis=1).loc[slicer[3,:,:]].unstack([0]).round(3)\n",
    "lee_beta3_summary.index = [f'{iii.replace(\" \", \"\")}_3' for iii in lee_beta3_summary.index]\n",
    "    \n",
    "lee_beta1_summary.columns = pd.MultiIndex.from_tuples([('Lee et al.', 'Estimate'), ('Lee et al.', 'Estimated SE')])\n",
    "lee_beta2_summary.columns = pd.MultiIndex.from_tuples([('Lee et al.', 'Estimate'), ('Lee et al.', 'Estimated SE')])\n",
    "lee_beta3_summary.columns = pd.MultiIndex.from_tuples([('Lee et al.', 'Estimate'), ('Lee et al.', 'Estimated SE')])\n",
    "\n",
    "beta_summary_comparison = pd.concat([lee_beta1_summary, lee_beta2_summary, lee_beta3_summary], axis=0)\n",
    "\n",
    "twostep_beta1_summary.columns = pd.MultiIndex.from_tuples([('two-step', 'Estimate'), ('two-step', 'Estimated SE')])\n",
    "twostep_beta2_summary.columns = pd.MultiIndex.from_tuples([('two-step', 'Estimate'), ('two-step', 'Estimated SE')])\n",
    "twostep_beta3_summary.columns = pd.MultiIndex.from_tuples([('two-step', 'Estimate'), ('two-step', 'Estimated SE')])\n",
    "\n",
    "reg_twostep_beta1_summary.columns = pd.MultiIndex.from_tuples([('Regularized two-step', 'Estimate'), ('Regularized two-step', 'Estimated SE')])\n",
    "reg_twostep_beta2_summary.columns = pd.MultiIndex.from_tuples([('Regularized two-step', 'Estimate'), ('Regularized two-step', 'Estimated SE')])\n",
    "reg_twostep_beta3_summary.columns = pd.MultiIndex.from_tuples([('Regularized two-step', 'Estimate'), ('Regularized two-step', 'Estimated SE')])\n",
    "\n",
    "tmp = pd.concat([twostep_beta1_summary.round(3), twostep_beta2_summary.round(3), twostep_beta3_summary.round(3)], axis=0)\n",
    "tmp2 = pd.concat([reg_twostep_beta1_summary.round(3), reg_twostep_beta2_summary.round(3), reg_twostep_beta3_summary.round(3)], axis=0)\n",
    "\n",
    "beta_summary_comparison = pd.concat([beta_summary_comparison, tmp, tmp2], axis=1)\n",
    "beta_summary_comparison.index.name =  r'$\\beta_{jk}$'\n",
    "beta_summary_comparison.index = [c.replace(\"_\", \" \") for c in beta_summary_comparison.index]\n",
    "beta_summary_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk1_rename_index_dict = {k + f' 1': v for k, v in rename_beta_index.items()}\n",
    "risk1 = beta_summary_comparison.iloc[:int(len(beta_summary_comparison) // 3)].rename(risk1_rename_index_dict, axis=0)\n",
    "print(risk1.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk2_rename_index_dict = {k + f' 2': v for k, v in rename_beta_index.items()}\n",
    "risk2 = beta_summary_comparison.iloc[int(len(beta_summary_comparison) // 3):2*(int(len(beta_summary_comparison) // 3))].rename(risk2_rename_index_dict, axis=0)\n",
    "print(risk2.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk3_rename_index_dict = {k + f' 3': v for k, v in rename_beta_index.items()}\n",
    "risk3 = beta_summary_comparison.iloc[2*int(len(beta_summary_comparison) // 3):].rename(risk3_rename_index_dict, axis=0)\n",
    "print(risk3.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'mimic_summary_.png'\n",
    "\n",
    "first_model_name = 'Lee et al.'\n",
    "second_model_name = 'two-step'\n",
    "times = range(1, ADMINISTRATIVE_CENSORING+1)\n",
    "\n",
    "lee_colors = ['tab:blue', 'tab:green', 'tab:red']\n",
    "two_step_colors = ['navy', 'darkgreen', 'tab:brown']\n",
    "true_colors = ['tab:blue', 'tab:green', 'tab:red']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "counts = fitters_table.groupby(['J', 'X'])['pid'].count().unstack('J').fillna(0)\n",
    "\n",
    "two_step_alpha_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_two_step_alpha.csv'), \n",
    "                                         index_col=['J', 'X'])\n",
    "\n",
    "lee_alpha_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_lee_alpha.csv'),\n",
    "                                   index_col=[0,1,2])\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "\n",
    "for j in [1, 2, 3]:\n",
    "\n",
    "    tmp_alpha = lee_alpha_k_results.loc[slicer[j, COEF_COL, :]].mean(axis=1)\n",
    "    tmp_alpha.index = [int(idx.split(')[')[1].split(']')[0]) for idx in tmp_alpha.index]\n",
    "    tmp_alpha = pd.Series(tmp_alpha.values.squeeze().astype(float), index=tmp_alpha.index)\n",
    "\n",
    "    ax.scatter(tmp_alpha.index, tmp_alpha.values,\n",
    "       label=f'J={j} ({first_model_name})', color=lee_colors[j-1], marker='o', alpha=0.4, s=40)\n",
    "\n",
    "    tmp_alpha = two_step_alpha_k_results.loc[slicer[j, 'alpha_jt']]\n",
    "    ax.scatter(tmp_alpha.index, tmp_alpha.values,\n",
    "       label=f'J={j} ({second_model_name})', color=two_step_colors[j-1], marker='*', alpha=0.7, s=20)\n",
    "\n",
    "    ax.set_xlabel(r'Time', fontsize=18)\n",
    "    ax.set_ylabel(r'$\\alpha_{jt}$', fontsize=18)\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "\n",
    "ax.set_ylim([-13, 3])\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(counts.index, counts[1].values.squeeze(), label='J=1', color='navy', alpha=0.4, width=0.4)\n",
    "ax2.bar(counts.index, counts[2].values.squeeze(), label='J=2', color='darkgreen', alpha=0.4, align='edge',\n",
    "        width=0.4)\n",
    "ax2.bar(counts.index, counts[3].values.squeeze(), label='J=3', color='tab:red', alpha=0.6, align='edge',\n",
    "        width=-0.4)\n",
    "ax2.legend(loc='upper center', fontsize=12)\n",
    "ax2.set_ylabel('Number of observed events', fontsize=16, color='red')\n",
    "ax2.tick_params(axis='y', colors='red')\n",
    "ax2.set_ylim([0, 8000])\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "if filename is not None:\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR, filename), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "penalizers = np.arange(-14, -3.9, step=step) \n",
    "n_splits = 4\n",
    "seed = 1\n",
    "cross_validators = {}\n",
    "\n",
    "for idp, penalizer in enumerate(penalizers):\n",
    "    print(f\"Started Penalizer: {penalizer}, {idp+1}/{len(penalizers)}\")\n",
    "    fit_beta_kwargs = {\n",
    "            'model_kwargs': {\n",
    "            'penalizer': np.exp(penalizer),\n",
    "            'l1_ratio': 1\n",
    "        }\n",
    "    }\n",
    "    start = time()\n",
    "    cross_validators[penalizer] = TwoStagesCV()\n",
    "    cross_validators[penalizer].cross_validate(full_df=fitters_table, n_splits=n_splits, seed=seed, nb_workers=1, \n",
    "                                               fit_beta_kwargs=fit_beta_kwargs,\n",
    "                                               metrics=['PE', 'AUC', 'IAUC', 'GAUC'])\n",
    "    end = time()\n",
    "    print(f\"Finished Penalizer: {penalizer}, {idp+1}/{len(penalizers)}, {int(end-start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "cross_validator_null = TwoStagesCV()\n",
    "cross_validator_null.cross_validate(full_df=fitters_table, n_splits=n_splits, seed=seed, nb_workers=1, \n",
    "                                    metrics=['PE', 'AUC', 'IAUC', 'GAUC'])\n",
    "end = time()\n",
    "print(f\"Finished {int(end-start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_censoring = (100*len(fitters_table[(fitters_table['J'] == 0) & (fitters_table['X'] <= ADMINISTRATIVE_CENSORING)]) / len(fitters_table))\n",
    "adm_censoring = (100*len(fitters_table[(fitters_table['J'] == 0) & (fitters_table['X'] > ADMINISTRATIVE_CENSORING)]) / len(fitters_table))\n",
    "risks = (100*fitters_table.groupby(['J']).size() / fitters_table.groupby('J').size().sum()).round(1)\n",
    "print(f\"LOF censoring: {lof_censoring:.1f}%, Administrative censoring: {adm_censoring:.1f}%, Home: {risks.loc[1]}%, Further treatment: {risks.loc[2]}%, Death: {risks.loc[3]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticksize = 15\n",
    "axes_title_fontsize = 17\n",
    "legend_size = 13\n",
    "\n",
    "risk_names = ['Home', 'Further Treatment', 'Death']\n",
    "risk_colors = ['tab:blue', 'tab:green', 'tab:red']\n",
    "risk_letters = ['d', 'e', 'f', 'g', 'h', 'i']\n",
    "chosen_lambda = -7\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 17))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "add_panel_text(ax, 'a')\n",
    "ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=ticksize)\n",
    "ax.set_xlabel(r'Log ($\\lambda$)', fontsize=axes_title_fontsize)\n",
    "ax.set_ylabel(r'Global AUC', fontsize=axes_title_fontsize)\n",
    "\n",
    "penalizers_x, mean_gauc, std_gauc = [], [], []\n",
    "for penalizer in sorted(cross_validators.keys()):\n",
    "    ser = pd.Series(cross_validators[penalizer].global_auc)\n",
    "    penalizers_x.append(penalizer)\n",
    "    mean_gauc.append(ser.mean())\n",
    "    std_gauc.append(ser.std())\n",
    "\n",
    "ax.errorbar(penalizers_x, mean_gauc, yerr=std_gauc, fmt=\"o\", color='g', alpha=0.5, label='With Penalization')\n",
    "ax.axhline(pd.Series(cross_validator_null.global_auc).mean(), ls = '--', label='Without Penalization', color='tab:blue')\n",
    "ax.axvline(chosen_lambda, color='brown', ls='-.', label=r'Chosen $\\lambda$')\n",
    "ax.legend(fontsize=legend_size)\n",
    "ax.set_ylim([0.53, 0.78])\n",
    "\n",
    "\n",
    "ax = axes[0, 1]\n",
    "add_panel_text(ax, 'b')\n",
    "ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=ticksize)\n",
    "ax.set_xlabel(r'Log ($\\lambda$)', fontsize=axes_title_fontsize)\n",
    "ax.set_ylabel(r'Integrated AUC', fontsize=axes_title_fontsize)\n",
    "\n",
    "fig_mean = pd.DataFrame()\n",
    "fig_std = pd.DataFrame()\n",
    "for p in sorted(cross_validators.keys()):\n",
    "    iauc_df = pd.DataFrame.from_dict(cross_validators[p].integrated_auc)\n",
    "    mean_ser = pd.DataFrame.from_dict(cross_validators[p].integrated_auc).mean(axis=1)\n",
    "    mean_ser.name = penalizer\n",
    "    std_ser = pd.DataFrame.from_dict(cross_validators[p].integrated_auc).std(axis=1)\n",
    "    std_ser.name = penalizer\n",
    "    fig_mean = pd.concat([fig_mean, mean_ser], axis=1)\n",
    "    fig_std = pd.concat([fig_std, std_ser], axis=1)\n",
    "\n",
    "for risk in range(1,4):\n",
    "    ax.errorbar(penalizers_x, fig_mean.loc[risk], yerr=fig_std.loc[risk], fmt=\"o\", color=risk_colors[risk-1], alpha=0.5, label=f'{risk_names[risk-1]} - With Penalization')\n",
    "    ax.axhline(pd.DataFrame.from_dict(cross_validator_null.integrated_auc).mean(axis=1).loc[risk], ls = '--', label=f'{risk_names[risk-1]} - Without Penalization', color=risk_colors[risk-1])\n",
    "ax.set_ylim([0.48, 0.78])\n",
    "ax.axvline(chosen_lambda, color='brown', ls='-.', label=r'Chosen $\\lambda$')\n",
    "ax.legend(loc='lower left', fontsize=legend_size)\n",
    "\n",
    "for risk in range(1, 4):\n",
    "    for idp, penalizer in enumerate(cross_validators.keys()):\n",
    "\n",
    "        tmp_j1_params_df = pd.DataFrame()\n",
    "        for i_fold in range(n_splits):\n",
    "            tmp_j1_params_df = pd.concat([tmp_j1_params_df, cross_validators[penalizer].models[i_fold].beta_models[risk].params_], axis=1)\n",
    "\n",
    "        ser_1 = tmp_j1_params_df.mean(axis=1) \n",
    "        ser_1.name = penalizer\n",
    "\n",
    "        if idp == 0:\n",
    "            j1_params_df = ser_1.to_frame()\n",
    "        else:\n",
    "            j1_params_df = pd.concat([j1_params_df, ser_1], axis=1)\n",
    "\n",
    "\n",
    "    ax = axes[1, risk-1]\n",
    "    add_panel_text(ax, risk_letters[risk-1])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=ticksize)\n",
    "    for i in range(len(j1_params_df)):\n",
    "        ax.plot(penalizers_x, j1_params_df.iloc[i].values, lw=1)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f'{n_splits}-Fold Mean Coefficient Value', fontsize=axes_title_fontsize)\n",
    "            ax.set_xlabel(r'Log ($\\lambda$)', fontsize=axes_title_fontsize)\n",
    "            ax.set_title(rf'$\\beta_{risk}$ - {risk_names[risk-1]}', fontsize=axes_title_fontsize)\n",
    "            ax.axvline(chosen_lambda, color='tab:blue', alpha=1, ls='--', lw=1)\n",
    "\n",
    "    ax = axes[0, 2]\n",
    "    \n",
    "    for idp, penalizer in enumerate(cross_validators.keys()):\n",
    "        tmp_ser = j1_params_df[penalizer].round(3)\n",
    "        count = (tmp_ser.abs() > 0).sum()\n",
    "        if idp == 0:\n",
    "            ax.scatter(penalizer, count, color=risk_colors[risk-1], alpha=0.8, marker='P', label=f'{risk_names[risk-1]}')\n",
    "        else:\n",
    "            ax.scatter(penalizer, count, color=risk_colors[risk-1], alpha=0.8, marker='P')\n",
    "        if penalizer == chosen_lambda:\n",
    "            print(f\"Risk {risk}: {count} non-zero coefficients at chosen lambda {chosen_lambda}\")\n",
    "\n",
    "add_panel_text(ax, 'c')\n",
    "ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=ticksize)\n",
    "ax.set_xlabel(r'Log ($\\lambda$)', fontsize=axes_title_fontsize)\n",
    "ax.set_ylabel(f'Number of Non-Zero Coefficient', fontsize=axes_title_fontsize)\n",
    "ax.axvline(chosen_lambda, color='tab:blue', alpha=1, ls='--', lw=1)\n",
    "ax.legend(loc='lower left', fontsize=legend_size)\n",
    "\n",
    "for risk in range(1, 4):\n",
    "    ax = axes[2, risk-1]\n",
    "    add_panel_text(ax, risk_letters[3+risk-1])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=ticksize)\n",
    "    mean_auc = cross_validators[chosen_lambda].results.loc[slicer['AUC', :, risk]].mean()\n",
    "    std_auc = cross_validators[chosen_lambda].results.loc[slicer['AUC', :, risk]].std()\n",
    "    ax.errorbar(mean_auc.index, mean_auc.values, yerr=std_auc.values, fmt=\"o\", color=risk_colors[risk-1], alpha=0.8)\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_yticklabels([c.round(1) for c in np.arange(0, 1.1, 0.1)])\n",
    "    ax.set_xlabel(r'Time', fontsize=axes_title_fontsize)\n",
    "    ax.set_ylabel(f'AUC (t)', fontsize=axes_title_fontsize)\n",
    "    ax.set_title(fr'{risk_names[risk-1]}, Log ($\\lambda$) = {chosen_lambda}', fontsize=axes_title_fontsize)\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.axhline(0.5, ls='--', color='k', alpha=0.3)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.bar(counts.index, counts[risk].values.squeeze(), color=risk_colors[risk-1], alpha=0.8, width=0.4)\n",
    "    ax2.set_ylabel('Number of observed events', fontsize=axes_title_fontsize, color=risk_colors[risk-1])\n",
    "    ax2.tick_params(axis='y', colors=risk_colors[risk-1])\n",
    "    ax2.set_ylim([0, 5100])\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "    ax2.tick_params(axis='both', which='minor', labelsize=ticksize)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(OUTPUT_DIR, 'mimic_regularization_fig.png'), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mimic_summary_.png'\n",
    "\n",
    "first_model_name = 'Lee et al.'\n",
    "second_model_name = 'two-step'\n",
    "third_model_name = 'Regularized two-step'\n",
    "times = range(1, ADMINISTRATIVE_CENSORING+1)\n",
    "\n",
    "lee_colors = ['tab:blue', 'tab:green', 'tab:red']\n",
    "two_step_colors = ['navy', 'darkgreen', 'tab:brown']\n",
    "reg_two_step_colors = ['darkviolet', 'olive', 'maroon']\n",
    "true_colors = ['tab:blue', 'tab:green', 'tab:red']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "counts = fitters_table.groupby(['J', 'X'])['pid'].count().unstack('J').fillna(0)\n",
    "\n",
    "two_step_alpha_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_two_step_alpha.csv'), \n",
    "                                         index_col=['J', 'X'])\n",
    "\n",
    "lee_alpha_k_results = pd.read_csv(os.path.join(OUTPUT_DIR, f'{case}_lee_alpha.csv'),\n",
    "                                   index_col=[0,1,2])\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "\n",
    "tmp_j1_params_df = pd.DataFrame()\n",
    "for i_fold in range(n_splits):\n",
    "    tmp_j1_params_df = pd.concat([tmp_j1_params_df, cross_validators[chosen_lambda].models[i_fold].alpha_df.set_index(['J', 'X'])['alpha_jt']], axis=1)\n",
    "\n",
    "ser_1 = tmp_j1_params_df.mean(axis=1) \n",
    "ser_1.name = penalizer\n",
    "\n",
    "for j in [1, 2, 3]:\n",
    "\n",
    "    tmp_alpha = lee_alpha_k_results.loc[slicer[j, COEF_COL, :]].mean(axis=1)\n",
    "    tmp_alpha.index = [int(idx.split(')[')[1].split(']')[0]) for idx in tmp_alpha.index]\n",
    "    tmp_alpha = pd.Series(tmp_alpha.values.squeeze().astype(float), index=tmp_alpha.index)\n",
    "\n",
    "    ax.scatter(tmp_alpha.index, tmp_alpha.values,\n",
    "       label=f'J={j} ({first_model_name})', color=lee_colors[j-1], marker='o', alpha=0.4, s=40)\n",
    "\n",
    "    tmp_alpha = two_step_alpha_k_results.loc[slicer[j, 'alpha_jt']]\n",
    "    ax.scatter(tmp_alpha.index, tmp_alpha.values,\n",
    "       label=f'J={j} ({second_model_name})', color=two_step_colors[j-1], marker='*', alpha=0.7, s=20)\n",
    "\n",
    "    ax.scatter(range(1, ADMINISTRATIVE_CENSORING+1), ser_1.loc[slicer[j, :]].values,\n",
    "       label=f'J={j} ({third_model_name})', color=reg_two_step_colors[j-1], marker='>', alpha=0.7, s=20)\n",
    "\n",
    "    ax.set_xlabel(r'Time', fontsize=18)\n",
    "    ax.set_ylabel(r'$\\alpha_{jt}$', fontsize=18)\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "\n",
    "\n",
    "ax.set_ylim([-13, 4.5])\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(counts.index, counts[1].values.squeeze(), label='J=1', color='navy', alpha=0.4, width=0.4)\n",
    "ax2.bar(counts.index, counts[2].values.squeeze(), label='J=2', color='darkgreen', alpha=0.4, align='edge',\n",
    "        width=0.4)\n",
    "ax2.bar(counts.index, counts[3].values.squeeze(), label='J=3', color='tab:red', alpha=0.6, align='edge',\n",
    "        width=-0.4)\n",
    "ax2.legend(loc='upper center', fontsize=12)\n",
    "ax2.set_ylabel('Number of observed events', fontsize=16, color='red')\n",
    "ax2.tick_params(axis='y', colors='red')\n",
    "ax2.set_ylim([0, 8500])\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "if filename is not None:\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR, filename), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
