# This file presents an R implementation exemplifying the estimation method 
# presented in Meir and Gorfine (2023) [1]: 
#
# The preprint of the paper is available at: https://arxiv.org/abs/2303.01186
#
# The process commences with the sampling of a dataset comprising two competing 
# events and implements right censoring based on defined model parameters. 
#
# Subsequently, the implementation demonstrates the estimation of the model 
# using the data, utilizing two methods: 
# (i) Meir and Gorfine (2023) [1]. 
# (ii) Lee et al. (2018) [2].
#
# Lastly, a comparative analysis of the results generated by these methods is 
# provided in the output file for reference and evaluation.

# References
# ----------
# [1] Meir, T and Gorfine, M,"Discrete-time Competing-Risks Regression 
#     with or without Penalization", 2023.
# [2] Lee M, Feuer EJ, Fine JP. On the analysis of discrete time competing risks 
#     data. Biometrics. 2018;74(4):1468-1481. doi:10.1111/biom.12881


# Definitions
# ------------
rm(list = ls())
library(tidyverse)
library(survival)


# Output filename (provide full path, or it will be saved in current working directory):
outputfile <- "results_comparison.csv"


# Number of observations to sample n:
n <- 250


# Number of Discrete-times
d <- 7
times <- c(1:d)
maxt <- length(times)


# Definition of the true logit link function model parameters (see [1])
beta11 <- -0.7*c(log(0.8),log(3),log(3),log(2.5),log(2))
beta12 <- -0.6*c(log(1),log(3),log(4),log(3),log(2))
alpha1 <- -1.4 + 0.4*log(times)
alpha2 <- -1.3 + 0.4*log(times)


# Number of features p (the length of the Z vector) 
p <- length(beta11) 


# Number of repetitions
rep <- 200
dim.time <- length(times)


# In the following function, we implement the data sampling procedure.
# First, we compute the hazard functions for each observation based on its 
# associated covariates. Next, we determine the overall survival function for 
# each observation, the probability of occurrence for each event, and the 
# marginal probability for each event. 

# For each observation, we sample the observed event given the marginal 
# probability associated with each event. Then, given the sampled observed event, 
# we proceed to sample the event time from the conditional probability of the 
# event occurrence.

sampling.discrete.competing.Cox <- function(i) {
  # Covariates sampling
  z <- runif(5,0,1) 
  
  # Beta*Z calculation
  bz1 <- as.vector(beta11%*%z)
  bz2 <- as.vector(beta12%*%z)  
  
  # Hazard function J=1
  exp1t <- exp(alpha1+bz1)/(1+exp(alpha1+bz1)) 
  
  # Hazard function J=2
  exp2t <- exp(alpha2+bz2)/(1+exp(alpha2+bz2)) 
  
  # Overall survival function
  st <- c(1,cumprod(1-exp1t-exp2t)[1:(dim.time-1)]) 
  
  # Probability of event occurrence function J=1
  prob1t <- exp1t*st 
  
  # Probability of event occurrence function J=2
  prob2t <- exp2t*st 
  
  # Marginal probability J=1
  p1 <- sum(prob1t) 
  
  # Marginal probability J=2
  p2 <- sum(prob2t)
  
  # Conditional probability for event occurrence at t given the event type J=1
  prob1t.j1 <- prob1t/p1
  
  # Conditional probability for event occurrence at t given the event type J=2
  prob2t.j2 <- prob2t/p2 
  
  # Probability for no observed event by Tmax
  p0 <- 1-p1-p2
  
  # Event type sampling
  j <- sample(c(0,1,2),1,prob=c(p0,p1,p2)) 

  # Event time sampling
  if (j==0) {
    tt <- max(times)+1
    datai <- c(tt,0,z)
  } else 
  {
    if (j==1) { 
      tt <- sample(times,1,prob = prob1t.j1)
      datai <- c(tt,j,z)
    } else {
      tt <- sample(times,1,prob = prob2t.j2)
      datai <- c(tt,j,z)
    }
  }
  
  # Right censoring time sampling - random censoring
  cen.prob <- c(rep(0.02,max(times)),1-sum(rep(0.02,max(times))))
  cen <- sample(1:(max(times)+1),1,prob=cen.prob) 

  # Update event time and type in case of censored observation
  if (cen < tt)   {
    datai <- c(cen,0,z)
  }
  return(datai)
}


# In the following function we generate the expanded data as explained in [1, 2]
data.expansion <- function(n, dat) {
  Expdata <- NULL
  for (i in 1:n) {
    for (j in 1:dat$time[i]) {
      if (dat$time[i]==j & dat$status[i]==1) {
        delta1 = 1; delta2 = 0;
      } else {
        if (dat$time[i]==j & dat$status[i]==2) {
          delta1 = 0; delta2 =1;
        } else {
          delta1 = delta2 = 0;
        }
      }
      Expdata <- rbind(Expdata,c(dat$ID[i],j,delta1,delta2,1-delta1-delta2,dat$cov1[i],dat$cov2[i],dat$cov3[i],dat$cov4[i],dat$cov5[i]))
    }
  }
  Expdata <- as.data.frame(Expdata)
  names(Expdata) <- c("ID","time","delta1","delta2","delta3","cov1","cov2","cov3","cov4","cov5")
  return(Expdata)
}

# The following function is the objective function for minimization as explained in [1]
obj_fun <- function(a0,j,t,ndat,prob1,prob2) {
  if (j==1) {
    temp <- filter(ndat,time>=t)
    probt <- exp(a0+temp$betaZ1)/(1+exp(a0+temp$betaZ1))
    dif <- (mean(probt)-prob1[t])^2     
  } else {
    temp <- filter(ndat,time>=t)
    probt <- exp(a0+temp$betaZ2)/(1+exp(a0+temp$betaZ2))
    dif <- (mean(probt)-prob2[t])^2  
  }
  return(dif)
}



cov1.hat <- matrix(0,nrow = rep,ncol = maxt+p); cov2.hat <- matrix(0,nrow = rep,ncol = maxt+p)
cov1.hat.simple <- matrix(0,nrow = rep,ncol = maxt+p); cov2.hat.simple <- matrix(0,nrow = rep,ncol = maxt+p)
SE1 <- matrix(0,nrow = rep,ncol = maxt+p); SE2 <- matrix(0,nrow = rep,ncol = maxt+p)
SE1.short <- matrix(0,nrow = rep,ncol = p); SE2.short <- matrix(0,nrow = rep,ncol = p)
coverage1.short <- matrix(0,nrow = rep,ncol = p); coverage2.short <- matrix(0,nrow = rep,ncol = p)

# Repetitions loop
for (sam in 1:rep) {
  # Data sampling
  dat <- lapply(1:n, sampling.discrete.competing.Cox)
  dat <- as.data.frame(t(matrix(unlist(dat),2+p,n)))
  names(dat) <- c("time","statusJ","cov1","cov2","cov3","cov4","cov5")
  dat$ID <- 1:n
  ndat <- as.tibble(dat)

  table(dat$time,dat$statusJ)
  
  # Data Expansion
  Expdata <- data.expansion(n,dat)
  
  # Estimation of the event specific GLMs based on Lee et al. (2018) [2] 
  fit1 <- summary(glm(formula = delta1 ~ as.factor(time) + cov1+cov2+cov3+cov4+cov5 -1, family = binomial(link = logit), data = Expdata))
  fit2 <- summary(glm(formula = delta2 ~ as.factor(time) + cov1+cov2+cov3+cov4+cov5 -1, family = binomial(link = logit), data = Expdata))
  cov1.hat[sam,] <- unname(fit1$coefficients[-(maxt+1),1])
  cov2.hat[sam,] <- unname(fit2$coefficients[-(maxt+1),1])
  SE1[sam,] <- unname(fit1$coefficients[-(maxt+1),2])
  SE2[sam,] <- unname(fit2$coefficients[-(maxt+1),2])

  
  
  # Estimation of the event specific beta_j coefficients using the approach of Meir and Gorfine (2023) [1]
  # Note that method = "approximate" refers to Breslow approximation for the likelihood function
  # Other options such as "efron" and "exact" are available
  
  fit1.short <- clogit(delta1 ~ cov1+cov2+cov3+cov4+cov5  + strata(time), method = "exact", data = Expdata)
  fit2.short <- clogit(delta2 ~ cov1+cov2+cov3+cov4+cov5  + strata(time), method = "exact", data = Expdata)
  SE1.short[sam,] <- unname(summary(fit1.short)$coef[, 'se(coef)'])
  SE2.short[sam,] <- unname(summary(fit2.short)$coef[, 'se(coef)'])
  
  coverage1.df.short <- data.frame(true=c(beta11), lower=c(fit1.short$coefficients - 1.96*SE1.short[sam,]), upper=c(fit1.short$coefficients + 1.96*SE1.short[sam,]))
  coverage1.short[sam,] <- as.numeric((coverage1.df.short$true >= coverage1.df.short$lower) & (coverage1.df.short$true <= coverage1.df.short$upper))
  
  coverage2.df.short <- data.frame(true=c(beta12), lower=c(fit2.short$coefficients - 1.96*SE2.short[sam,]), upper=c(fit2.short$coefficients + 1.96*SE2.short[sam,]))
  coverage2.short[sam,] <- as.numeric((coverage2.df.short$true >= coverage2.df.short$lower) & (coverage2.df.short$true <= coverage2.df.short$upper))
  
  
  ndat <- ndat %>% 
    mutate(betaZ1 =  cbind(cov1,cov2,cov3,cov4,cov5)%*%fit1.short$coefficients, 
           betaZ2 =  cbind(cov1,cov2,cov3,cov4,cov5)%*%fit2.short$coefficients) %>% 
    arrange(time) 
  prob1 <- vector(); prob2 <- vector()
  for (t in 1:maxt) {
    n.t <- dim(filter(ndat,time>=t))[1] 
    n.t.1 <- dim(filter(ndat,time==t,statusJ==1))[1]
    n.t.2 <- dim(filter(ndat,time==t,statusJ==2))[1]
    prob1[t] <- n.t.1/n.t
    prob2[t] <- n.t.2/n.t
  }
  
  # Estimation of the event specific alpha_jt coefficients using the approach of Meir and Gorfine (2023) [1]
  alpha.mat <- matrix(0,maxt,2)
  for (j in 1:2) {
    for (t in 1:maxt) {
      res <- nlminb(-3,obj_fun,j=j,t=t,ndat=ndat,prob1=prob1,prob2=prob2)
      alpha.mat[t,j] <- res$par
    }    
  }

  cov1.hat.simple[sam,] <- c(alpha.mat[,1],fit1.short$coefficients) 
  cov2.hat.simple[sam,] <- c(alpha.mat[,2],fit2.short$coefficients)    
  
}

# Create and write output file to disk

results.coef <- cbind(cov1.hat,cov2.hat)
results.coef.fast <- cbind(cov1.hat.simple,cov2.hat.simple)
true.coef <- c(alpha1,beta11,alpha2,beta12)
est.coef <- apply(results.coef,2,mean)
est.coef.fast <- apply(results.coef.fast,2,mean)

results.se <- cbind(SE1,SE2)
results.se.fast <- cbind(SE1.short,SE2.short)
empirical.se <- apply(results.coef,2,sd)
empirical.se.fast <- apply(results.coef.fast,2,sd)
est.se <- apply(results.se, 2, mean)
est.se.fast <- apply(results.se.fast, 2, mean)

cbind(true.coef, est.coef, est.coef.fast )
cbind(empirical.se,empirical.se.fast )
results.dat <- as.data.frame( cbind(true.coef, est.coef, est.se, est.coef.fast, empirical.se.fast) )

write.csv(results.dat, file = outputfile, row.names = FALSE)



beta.results.coef <- cbind(cov1.hat[, (maxt+1):(maxt+p)],cov2.hat[, (maxt+1):(maxt+p)])
beta.results.coef.fast <- cbind(cov1.hat.simple[, (maxt+1):(maxt+p)],cov2.hat.simple[, (maxt+1):(maxt+p)])
true.coef <- c(beta11,beta12)
beta.est.coef <- apply(beta.results.coef,2,mean)
beta.est.coef.fast <- apply(beta.results.coef.fast,2,mean)

beta.results.se <- cbind(SE1[, (maxt+1):(maxt+p)],SE2[, (maxt+1):(maxt+p)])
beta.results.se.fast <- cbind(SE1.short,SE2.short)
beta.empirical.se <- apply(beta.results.coef,2,sd)
beta.empirical.se.fast <- apply(beta.results.coef.fast,2,sd)
beta.est.se <- apply(beta.results.se, 2, mean)
beta.est.se.fast <- apply(beta.results.se.fast, 2, mean)

beta.coverage.results <- cbind(coverage1.short,coverage2.short)
beta.coverage.fast <- apply(beta.coverage.results, 2, mean)

results.dat <- as.data.frame( cbind(true.coef, beta.est.coef, beta.est.se, beta.est.coef.fast, beta.est.se.fast, beta.empirical.se.fast, beta.coverage.fast) )

write.csv(results.dat, file = paste0('beta_', outputfile), row.names = FALSE)
